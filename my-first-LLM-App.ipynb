{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My First LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following, I aim to leverage Language Models (LLMs) to address inquiries regarding our personal data. To achieve this, the initial step involves transferring the contents of our personal data into a vector database. This crucial step facilitates efficient searches for relevant sections within the text. Utilizing both our data and the text interpretation capabilities of LLMs, we can effectively respond to user questions.\n",
    "\n",
    "The implementation of our use case will heavily rely on the LangChain framework.\n",
    "\n",
    "To start the construction of my personal assistant, I am going to start answering the question about the current President of the USA, I feed the prompt with information from the Wikipedia article “President of USA”. The idea is that we are going to use GPT3.5 as a LLM and we want to enable it to answer questions that the LLM cannot know usuing context injection.\n",
    "\n",
    "When using context injection, we are not modifying the LLM, we focus on the prompt itself and inject relevant context into the prompt. So we need to think about how to provide the prompt with the right information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to focus on creating a vector store, which is a specialized type of data store designed for efficient storage and retrieval of large quantities of vector data. \n",
    "\n",
    "Vector databases excel at querying and retrieving subsets of data based on criteria like similarity measures or mathematical operations. The initial step involves converting text data into vectors, but simply storing them in a data frame and searching for similarities step by step would be slow.\n",
    "\n",
    "I wanto to emphasize the importance of indexing as the second key component of a vector database. Indexing enables efficient mapping of queries to the most relevant items in the vector store without the need to compute similarities between every query and document, significantly improving the search process.\n",
    "\n",
    "I am going to calculate the embeddings and storing them in a vector store. To do this, we are using suitable modules from LangChain and chroma as a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data that I want to use to answer the users’ questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# URL of the Wikipedia page to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/President_of_the_United_States'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the text on the page\n",
    "text = soup.get_text()\n",
    "text = text.replace('\\n', '')\n",
    "# Open a new file called 'output.txt' in write mode and store the file object in a variable\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the data and define how I want to split the data into text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the document\n",
    "with open('./output.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# define the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(    \n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Embeddings Model I want to use to calculate the embeddings for your text chunks and store them in a vector store (here: Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Local-Workplace\\Personal-LLMs-Projects\\My-First-LLM-App\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# define the embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# use the text chunks and the embeddings model to fill our vector store\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculate the embeddings for the user’s question, find similar text chunks in our vector store and use them to build our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe Biden'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "users_question = \"Who is the current President of the United States?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm(prompt_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "users_question = \"Who was the first President of the United States?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Madison (1809–1817)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "users_question = \"Who was the fourth President of the United States?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington, John Adams, Thomas Jefferson, James Madison, James Monroe'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "users_question = \"Who was the first five Presidents of the United States?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe last five presidents of the United States were: Donald Trump, Barack Obama, George W. Bush, Bill Clinton, and George H. W. Bush.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "users_question = \"Who was the last five Presidents of the United States?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm(prompt_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
